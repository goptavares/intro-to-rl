{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"colab":{"name":"IntroToRL_Notebook1_Demo.ipynb","provenance":[],"collapsed_sections":["k9FR51P32yPN"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"a46_dLTN2yO5","colab_type":"text"},"source":["# Introduction\n","\n","In this demo we will explore a very simple environment. Cheese World is a one-dimensional grid with 4 states and only 2 possible actions: left and right. Arriving at the goal state gives you a reward of 1. Moving left from the start state (state 1 in the figure) stays in the same place, and moving anywhere from the goal state (state 4 in the figure) ends the episode.\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/CheeseWorld.png?raw=true\" width=680 height=200>\n","\n","This demo uses a simple random policy to traverse the environment and get to the goal. The value for each state-action pair is calculated with a simple additive strategy, i.e., by adding the rewards obtained for that pair throughout the episode.\n","\n","Run all cells and familiarize yourself  with the code, as it will help your write your own code to solve the exercises."]},{"cell_type":"markdown","metadata":{"id":"k9FR51P32yPN","colab_type":"text"},"source":["# Notebook setup"]},{"cell_type":"code","metadata":{"id":"aZ_YeYU02yPV","colab_type":"code","colab":{}},"source":["% matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from pylab import *\n","\n","!git clone https://github.com/goptavares/intro-to-rl.git\n","%run intro-to-rl/RL_worlds.py\n","%run intro-to-rl/plot_util.py"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VXqQ_vhL2yRm","colab_type":"text"},"source":["# Helper functions"]},{"cell_type":"code","metadata":{"id":"pAD_5vmq2yR8","colab_type":"code","colab":{}},"source":["def default_params(environment):\n","  \"\"\"\n","  Define the default parameters.\n","  Args:\n","    environment: an object corresponding to the environment.\n","  Returns:\n","    a dictionary containing the default parameters, where the keys\n","        are strings (parameter names).\n","  \"\"\"\n","  params = dict()\n","  params['environment'] = environment\n","  \n","  params['alpha'] = 0.1  # learning rate\n","  params['beta'] = 10  # inverse temperature\n","  params['epsilon'] = 0.05  # epsilon-greedy policy\n","  params['epsilon_decay'] = 0.9\n","  params['gamma'] = 1.0  # no discounting\n","\n","  return params"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d7MmIgt_0qAc","colab_type":"code","colab":{}},"source":["def init_state():\n","  # In Cheese World, the initial state is 0.\n","  return 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tk7sjaHMzt1N","colab_type":"code","colab":{}},"source":["def update_state(state, action, params):\n","  \"\"\"\n","  State transition based on world, action and current state.\n","  Args:\n","    state: integer corresponding to the current state.\n","    action: integer corresponding to the action taken.\n","    params: a dictionary containing the default parameters.\n","  Returns:\n","    an integer corresponding to the next state;\n","    an integer corresponding to the reward received.\n","  \"\"\"\n","  next_state, reward = params['environment'].get_outcome(state, action)\n","  return next_state, reward"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hiGufcKk46Bd","colab_type":"text"},"source":["# Random policy and simple additive value update"]},{"cell_type":"code","metadata":{"id":"pagB4h1rzu-a","colab_type":"code","colab":{}},"source":["def call_policy(params):\n","  # Random policy.\n","  return randint(params['environment'].n_actions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OKx-n5pI1Ae9","colab_type":"code","colab":{}},"source":["def update_value(prev_state, action, reward, value):\n","  \"\"\"\n","  Update the value function.\n","  Args:\n","    prev_state: an integer corresponding to the previous state.\n","    action: an integer correspoding to action taken.\n","    reward: a float corresponding to the reward received.\n","    value: a matrix indexed by state and action. \n","  Returns:\n","    the updated value function (matrix indexed by state and action).\n","  \"\"\"\n","  # Additive value update.\n","  value[prev_state, action] += reward\n","  return value"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3oRdyr1iwjtj","colab_type":"text"},"source":["# Explore Cheese World"]},{"cell_type":"code","metadata":{"id":"BvC5KNmC5PqI","colab_type":"code","colab":{}},"source":["def run_learning(value, params, n_episodes, max_steps):\n","  \"\"\"\n","  Args:\n","    value: a matrix indexed by state and action.\n","    params: a dictionary containing the default parameters.\n","    n_episodes: integer, number of episodes to run.\n","    max_steps: integer, maximum number of steps to take in each episode.\n","  Returns:\n","    a dictionary where the keys are integers (episode numbers)\n","        and the values are integers (total rewards per episode);\n","    the updated value function (matrix indexed by state and action).\n","  \"\"\"\n","  reward_sums = np.zeros(n_episodes)\n","\n","  # Loop over episodes.\n","  for episode in xrange(n_episodes):\n","    # Initialize state.\n","    state = init_state()    \n","    step = 0\n","    reward_sum = 0\n","\n","    # Make sure to break after max number of steps.\n","    while step < max_steps:\n","      # Get action from policy.\n","      action = call_policy(params)  \n","      # Update state based on action.\n","      next_state, reward = update_state(state, action, params)\n","      # Update value function.\n","      value = update_value(state, action, reward, value)  \n","      state = next_state\n","      # Sum the rewards obtained.\n","      reward_sum += reward  \n","      step += 1\n","      if next_state == None:\n","        # Episode ends.\n","        break  \n","    reward_sums[episode] = reward_sum\n","  return reward_sums, value"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QCkQh4Tzws6C","colab_type":"code","colab":{}},"source":["env = cheese_world()\n","params = default_params(environment=env)\n","\n","# Decision-maker: choose parameter values.\n","params['epsilon'] = 0.01\n","params['alpha'] = 0.5\n","params['beta'] = 10\n","params['gamma'] = 0.8\n","\n","# Define number of episodes and maximum steps per episode.\n","n_episodes = 5\n","max_steps = 10\n","\n","# Initialization.\n","# Start with uniform value function.\n","value = np.ones((env.n_states, env.n_actions))\n","\n","# Run learning.\n","reward_sums, value = run_learning(value, params, n_episodes, max_steps)\n","\n","fig = plot_state_action_values(env, value)\n","fig = plot_heatmap_max_val(env, value)\n","fig = plot_rewards(n_episodes, reward_sums, average_range=1)"],"execution_count":0,"outputs":[]}]}