{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"IntroToRL_Notebook3_AK.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":["a46_dLTN2yO5","k9FR51P32yPN","VXqQ_vhL2yRm","1NnGEjmNDKOp","WGw7wPQuD3Lr","VP0O6bELESQ4","hW0-B-q2FA_g","gl0MU9RJMo3q","jVboMJIFLJh5","4YdkxHQ3LyY5","eRABjR6_Mf-F"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"a46_dLTN2yO5","colab_type":"text"},"source":["# Introduction\n","\n","In this tutorial we will learn about different environments, then implement policies, dynamic programming algorithms and temporal difference algorithms to study their performance.\n","\n","### N-armed bandit\n","\n","In an N-armed bandit task, there is only a single state. Our implementation is a 4-armed bandit, which means there are 4 available actions, and each one returns a different reward.\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/4ArmedBandit.png?raw=true\" width=500 height=225>\n","\n","### Cliff world\n","\n","In this 4x10 grid there are 40 states and 4 possible actions: right, up, left and down. Falling into the cliff incurs a negative reward of -100 and ends the episode; moving into any other state incurs a reward of -1; moving into the world borders stays in the same place; moving anywhere from the goal state (state G in the figure) ends the episode.\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/CliffWorld.png?raw=true\" width=600 height=280>\n","\n","### Quentin's world\n","\n","In this 10x10 grid there are 100 states and 4 possible actions: right, up, left and down. The start state is in green in the figure; moving into one of the red states incurs a reward of -1; moving into the world borders stays in the same place; moving into the goal state (yellow square in the upper right corner) gives you a reward of 1; and moving anywhere from the goal state ends the episode.\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/QuentinsWorld.png?raw=true\" width=300 height=300>\n","\n","### Multi-room windy gridworld with cliffs\n","\n","In this 12x14 grid there are 168 states and 4 possible actions: right, up, left and down. The start state is marked with an S in the figure, and there are 2 goals states marked with a G. Each goal state is inside a room (the room walls are marked with darker lines). Moving into a goal states gives you a reward of 100. Moving into a wall or outside the world borders stays in the same place. The two rooms are windy, which means the resultant next states inside the rooms are shifted; the wind direction is indicated by a blue arrow, and the wind strength (size of the shift) in each column is indicated by a number between 0 and 2. There are also two cliffs marked in gray; falling into a cliff incurs a reward of -100 and ends the episode.\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/gridworld.png?raw=true\" width=451.2 height=447.2>\n"]},{"cell_type":"markdown","metadata":{"id":"k9FR51P32yPN","colab_type":"text"},"source":["# Notebook setup"]},{"cell_type":"code","metadata":{"id":"aZ_YeYU02yPV","colab_type":"code","colab":{}},"source":["% matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from pylab import *\n","\n","!git clone https://github.com/goptavares/intro-to-rl.git\n","%run intro-to-rl/RL_worlds.py\n","%run intro-to-rl/plot_util.py"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VXqQ_vhL2yRm","colab_type":"text"},"source":["# Helper functions"]},{"cell_type":"code","metadata":{"id":"pAD_5vmq2yR8","colab_type":"code","colab":{}},"source":["def default_params(environment):\n","  \"\"\"\n","  Define the default parameters.\n","  Args:\n","    environment: an object corresponding to the environment.\n","  Returns:\n","    a dictionary containing the default parameters, where the keys\n","        are strings (parameter names).\n","  \"\"\"\n","  params = dict()\n","  params['environment'] = environment\n","  \n","  params['alpha'] = 0.1  # learning rate\n","  params['beta'] = 10  # inverse temperature\n","  params['epsilon'] = 0.05  # epsilon-greedy policy\n","  params['epsilon_decay'] = 0.9\n","  params['gamma'] = 1.0  # no discounting\n","\n","  params['policy'] = 'epsilon_greedy'\n","  params['learning_rule'] = 'q_learning'\n","\n","  if environment.name == 'n_armed_bandit':\n","    params['gamma'] = 0.9\n","  elif environment.name == 'cliff_world':\n","    params['gamma'] = 1.0  # no discounting\n","  elif environment.name == 'quentins_world':\n","    params['gamma'] = 0.9\n","  elif environment.name == 'windy_cliff_grid':\n","    params['gamma'] = 0.8\n","\n","  return params"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d7MmIgt_0qAc","colab_type":"code","colab":{}},"source":["def init_state(params):\n","  \"\"\"\n","  Initialize the state at the beginning of an episode.\n","  Args:\n","    params: a dictionary containing the default parameters.\n","  Returns:\n","    an integer corresponding to the initial state.\n","  \"\"\"\n","  if params['environment'].name == 'windy_cliff_grid':\n","    return 0\n","  elif params['environment'].name == 'n_armed_bandit':\n","    return 0\n","  elif params['environment'].name == 'cliff_world':\n","    return 0\n","  elif params['environment'].name == 'quentins_world':\n","    return 54"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tk7sjaHMzt1N","colab_type":"code","colab":{}},"source":["def update_state(state, action, params):\n","  \"\"\"\n","  State transition based on world, action and current state.\n","  Args:\n","    state: integer corresponding to the current state.\n","    action: integer corresponding to the action taken.\n","    params: a dictionary containing the default parameters.\n","  Returns:\n","    an integer corresponding to the next state;\n","    an integer corresponding to the reward received.\n","  \"\"\"\n","  next_state, reward = params['environment'].get_outcome(state, action)\n","  return next_state, reward"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B8_pYbYvBHFL","colab_type":"code","colab":{}},"source":["def call_policy(state, value, params):\n","  \"\"\"\n","  Call a policy to choose actions, given current state and value function.\n","  Args:\n","    state: integer corresponding to the current state.\n","    value: a matrix indexed by state and action.\n","    params: a dictionary containing the default parameters.\n","  Returns:\n","    an integer corresponding action chosen according to the policy.\n","  \"\"\"\n","  if params['policy'] == 'epsilon_greedy':\n","    return epsilon_greedy(state, value, params)\n","  elif params['policy'] == 'softmax':\n","    return softmax(state, value, params)\n","  else:\n","    # Random policy (if policy not recognized, choose randomly).\n","    return randint(params['environment'].n_actions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9vbwm29wBaE9","colab_type":"code","colab":{}},"source":["def update_value(prev_state, action, reward, state, value, params):\n","  \"\"\"\n","  Update the value function.\n","  Args:\n","    prev_state: an integer corresponding to the previous state.\n","    action: an integer correspoding to action taken.\n","    reward: a float corresponding to the reward received.\n","    state: an integer corresponding to the current state;\n","        should be None if the episode ended.\n","    value: a matrix indexed by state and action.\n","    params: a dictionary containing the default parameters. \n","  Returns:\n","    the updated value function (matrix indexed by state and action).\n","  \"\"\"\n","  if params['learning_rule'] == 'q_learning':\n","    # Off policy learning.\n","    return q_learning(prev_state, action, reward, state, value, params)\n","  elif params['learning_rule'] == 'sarsa':\n","    # On policy learning.\n","    return sarsa(prev_state, action, reward, state, value, params)\n","  else:\n","    print('Learning rule not recognized')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BvC5KNmC5PqI","colab_type":"code","colab":{}},"source":["def run_learning(value, params, n_episodes, max_steps):\n","  \"\"\"\n","  Args:\n","    value: a matrix indexed by state and action.\n","    params: a dictionary containing the default parameters.\n","    n_episodes: integer, number of episodes to run.\n","    max_steps: integer, maximum number of steps to take in each episode.\n","  Returns:\n","    a dictionary where the keys are integers (episode numbers)\n","        and the values are integers (total rewards per episode);\n","    the updated value function (matrix indexed by state and action).\n","  \"\"\"\n","  reward_sums = np.zeros(n_episodes)\n","\n","  # Loop over episodes.\n","  for episode in range(n_episodes):\n","    # Initialize state.\n","    state = init_state(params)    \n","    step = 0\n","    reward_sum = 0\n","\n","    # Make sure to break after max number of steps.\n","    while step < max_steps:\n","      # Get action from policy.\n","      action = call_policy(state, value, params)  \n","      # Update state based on action.\n","      next_state, reward = update_state(state, action, params)\n","      # Update value function.\n","      value = update_value(state, action, reward, next_state, value, params) \n","      state = next_state\n","      # Sum the rewards obtained.\n","      reward_sum += reward  \n","      step += 1\n","      if next_state == None:\n","        # Episode ends.\n","        break  \n","    reward_sums[episode] = reward_sum\n","  return reward_sums, value"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1NnGEjmNDKOp","colab_type":"text"},"source":["# Exercise 1: Policy Evaluation\n","\n","**Complete the policy evaluation algorithm below.**\n","\n","The function takes the policy to be evaluated, the default parameters and a threshold for the stopping criterion as input and returns the value function.\n"]},{"cell_type":"code","metadata":{"id":"4zPTVz1PDe3E","colab_type":"code","colab":{}},"source":["def policy_evaluation(policy, params, theta=0.000001):\n","  \"\"\"\n","  Evaluates a policy given an environment and a full description of the\n","      environment's dynamics.\n","  Args:\n","    policy: a matrix indexed by state and action, where each entry is a float\n","        corresponding to the probability of taking that action from that state.\n","    params: a dictionary containing the default parameters.\n","    theta: float; we stop evaluation once our value function change is less than\n","        theta for all states.\n","  Returns:\n","    the value function (vector indexed by state).\n","  \"\"\"\n","  env = params['environment']\n","  outcomes = env.get_all_outcomes()\n","  \n","  # Start with a random (all zeros) value function.\n","  value = np.zeros(env.n_states)\n","\n","  while True:\n","    delta = 0\n","    # For each state, perform a \"full backup\".\n","    for state in range(env.n_states):\n","      v = 0\n","      # Look at all possible next actions.\n","      for action, action_prob in enumerate(policy[state]):\n","        # For each action, look at all possible next states.\n","        for prob, next_state, reward in outcomes[state, action]:\n","          if next_state == None:\n","              value_next = 0\n","          else:\n","              value_next = value[next_state]\n","          # Calculate the expected value.\n","          v += action_prob * prob * (reward + params['gamma'] * value_next)\n","      # How much our value function changed (across any states).\n","      delta = max(delta, np.abs(value[state] - v))\n","      value[state] = v\n","    # Stop evaluating once our value function change is below the threshold.\n","    if delta < theta:\n","        break\n","  return value"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WGw7wPQuD3Lr","colab_type":"text"},"source":["# Exercise 2: Policy Iteration\n","\n","**Complete the policy iteration algorithm below.**\n","\n","The function takes the default parameters and a threshold for the stopping criterion as input and returns the optimal policy and the value function for the optimal police. It uses the policy evaluation function written in Exercise 1."]},{"cell_type":"code","metadata":{"id":"dCoVhtIaEAfe","colab_type":"code","colab":{}},"source":["def policy_iteration(params, theta=0.000001):\n","  \"\"\"\n","  Iteratively evaluates and improves a policy until an optimal policy is found.\n","  Args:\n","    params: a dictionary containing the default parameters.\n","    theta: float, to be used in policy_evaluation; we stop evaluation once our\n","        value function change is less than theta for all states.\n","  Returns:\n","    the optimal policy (matrix indexed by state and action);\n","    the corresponding value function (vector indexed by state).\n","  \"\"\"\n","  env = params['environment']\n","  outcomes = env.get_all_outcomes()\n","\n","  # Start with a random policy.\n","  policy = np.ones([env.n_states, env.n_actions]) / env.n_actions\n","  \n","  while True:\n","    # Evaluate the current policy.\n","    value = policy_evaluation(policy, params, theta)\n","\n","    # This will be set to false if we make any changes to the policy.\n","    policy_stable = True\n","    \n","    # Iterate over all states.\n","    for state in range(env.n_states):\n","      # The best action we would take under the current policy.\n","      chosen_action = np.argmax(policy[state])\n","      \n","      # Find the best action by one-step lookahead using the new value function.\n","      # Ties are resolved arbitarily.\n","      action_values = np.zeros(env.n_actions)\n","      for action in range(env.n_actions):\n","        for prob, next_state, reward in outcomes[state, action]:\n","          if next_state == None:\n","            value_next = 0\n","          else:\n","            value_next = value[next_state]\n","          action_values[action] += prob * (reward + params['gamma'] * value_next)\n","      best_action = np.argmax(action_values)\n","      \n","      # Greedily update the policy.\n","      if chosen_action != best_action:\n","          policy_stable = False\n","      policy[state] = np.eye(env.n_actions)[best_action]\n","\n","    # If the policy is stable, we've found an optimal policy.\n","    if policy_stable:\n","        break\n","  return policy, value"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VP0O6bELESQ4","colab_type":"text"},"source":["# Exercise 3: Value Iteration\n","\n","One drawback to policy iteration is that each of its iterations involves policy evaluation, which may be slow due to multiple sweeps through the state set. We can make this process faster by truncating the policy evaluation step of policy iteration, stopping it after just one sweep (one back-up of each state). This modified algorithm is called value iteration.\n","\n","**Complete the value iteration algorithm.**\n","\n","The function takes the default parameters and a threshold for the stopping criterion as input and returns the policy and corresponding value function. There is also a helper function to calculate the one-step lookahead, i.e., the value for all actions in a given state using the current estimate of the value function."]},{"cell_type":"code","metadata":{"id":"If92nFbuEhHi","colab_type":"code","colab":{}},"source":["def one_step_lookahead(state, value, params):\n","  \"\"\"\n","  Helper function to calculate the value for all actions in a given state.\n","  Args:\n","    state: int, the state to consider.\n","    value: the value function to use as an estimator (vector indexed by state).\n","    params: a dictionary containing the default parameters.\n","  Returns:\n","    a vector indexed by action containing the expected value of each action.\n","  \"\"\"\n","  env = params['environment']\n","  outcomes = env.get_all_outcomes()\n","  A = np.zeros(env.n_actions)\n","  for action in range(env.n_actions):\n","    for prob, next_state, reward in outcomes[state, action]:\n","      if next_state == None:\n","        value_next = 0\n","      else:\n","        value_next = value[next_state]\n","      A[action] += prob * (reward + params['gamma'] * value_next)\n","  return A\n","\n","def value_iteration(params, theta=0.000001):\n","  \"\"\"\n","  Value iteration algorithm.\n","  Args:\n","    params: a dictionary containing the default parameters.\n","    theta: float; we stop evaluation once our value function change is less\n","          than theta for all states.\n","  Returns:\n","    the optimal policy (matrix indexed by state and action);\n","    the corresponding value function (vector indexed by state).\n","  \"\"\"    \n","  env = params['environment']\n","  value = np.zeros(env.n_states)\n","\n","  while True:\n","    # Stopping condition.\n","    delta = 0\n","    # Update each state.\n","    for state in range(env.n_states):\n","      # Do a one-step lookahead to find the best action.\n","      A = one_step_lookahead(state, value, params)\n","      best_action_value = np.max(A)\n","      # Calculate delta across all states seen so far.\n","      delta = max(delta, np.abs(best_action_value - value[state]))\n","      # Update the value function.\n","      value[state] = best_action_value        \n","    # Check if we can stop.\n","    if delta < theta:\n","      break\n","  \n","  # Create a deterministic policy using the optimal value function.\n","  policy = np.zeros([env.n_states, env.n_actions])\n","  for state in range(env.n_states):\n","    # One step lookahead to find the best action for this state.\n","    A = one_step_lookahead(state, value, params)\n","    best_action = np.argmax(A)\n","    # Always take the best action.\n","    policy[state, best_action] = 1.0\n","\n","  return policy, value"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hW0-B-q2FA_g","colab_type":"text"},"source":["# Exercise 4\n","\n","The code below allows you to test the performance of policy iteration and value iteration for a selected world (try cliff world, Quentin's world or windy cliff grid). Use the functions provided in the plot_util module to:\n","\n","* Plot the action corresponding to the policy at each state;\n","* Plot the value associated with each state.\n","\n","**A.** Experiment with different parameter values.\n","\n","* Pick a range for the temporal discount factor  γ  and look at how the results change.\n","\n","**B.** To make sure that your algorithms have been implemented correctly, compare your results to the ones shown below.\n","\n","**C.** Compare the results obtained using policy iteration vs. value iteration. What differences do you notice between the two approaches?\n","\n","###Cliff world using policy improvement and  γ =0.9:\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/policyimp_actions.png?raw=true\" height=200 width=320>\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/policyimp_maxval.png?raw=true\" height=200 width=320>\n","\n","\n","###Quentin's world using using value iteration and  γ =0.9:\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/valueit_actions.png?raw=true\" height=250 width=250>\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/valueit_maxval.png?raw=true\" height=250 width=280>"]},{"cell_type":"code","metadata":{"id":"2aQTuM-CJqr-","colab_type":"code","colab":{}},"source":["# Choose a world.\n","env = cliff_world()\n","# env = quentins_world()\n","# env = windy_cliff_grid()\n","\n","params = default_params(environment=env)\n","\n","# Decision-maker: choose parameter values.\n","params['gamma'] = 0.9\n","\n","# Choose a DP algorithm: policy iteration or value iteration.\n","policy, value = policy_iteration(params)\n","# policy, value = value_iteration(params)\n","\n","fig = plot_quiver_max_action(env, policy)\n","fig = plot_heatmap_max_val(env, value)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gl0MU9RJMo3q","colab_type":"text"},"source":["# Quick refresher\n","\n","### Learning algorithms and policies\n","\n","**Learning algorithms:**\n","\n","*Sarsa (on-policy)*\n","\n","\\begin{align}\n","Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\big(r_t + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\\big)\n","\\end{align}\n","\n","with temporal discount rate $\\gamma$ and learning rate $\\alpha$.\n","\n","*Q-learning (off-policy)*\n","\n","\\begin{align}\n","Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\big(r_t + \\gamma\\max_\\limits{a} Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\\big)\n","\\end{align}\n","\n","\n","**Policies:**\n","\n","*Epsilon-greedy*\n","\n","\\begin{align}\n","P(a_t|s_t) = \\epsilon \\frac{1}{N_a}  + (1-\\epsilon)1[a_t =\\max_\\limits{a}Q(a_t,s_t)]\n","\\end{align}\n","\n","*Softmax*\n","\n","\\begin{align}\n","P(a_t|s_t) = \\frac{\\exp(Q(a_t,s_t)/\\tau)}{\\Sigma_{i=1}^n \\exp(Q(i)/\\tau)}\n","\\end{align}"]},{"cell_type":"markdown","metadata":{"id":"jVboMJIFLJh5","colab_type":"text"},"source":["# Exercise 5: Decision Policies\n","\n","**A.** Complete the epsilon-greedy policy function below.\n","\n","**B.** [Optional] Complete the softmax policy function below.\n","\n","Both functions take the current state, the value function and default parameters as input and return an action."]},{"cell_type":"code","metadata":{"id":"HS65pCX1LShL","colab_type":"code","colab":{}},"source":["def epsilon_greedy(state, value, params):\n","  \"\"\"\n","  Epsilon-greedy policy: selects the maximum value action with probability\n","      (1-epsilon) and selects randomly with epsilon probability.\n","  Args:\n","    state: an integer corresponding to the current state.\n","    value: a matrix indexed by state and action.\n","    params: a dictionary containing the default parameters. \n","  Returns:\n","    action: an integer corresponding action chosen according to the policy.\n","  \"\"\"\n","  value_now = value[state,:]\n","  if rand() > params['epsilon']:\n","    action = where(value_now == max(value_now))[0][0]\n","  else:\n","    action = randint(len(value_now))\n","  return action"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQtDl1WFLaHn","colab_type":"code","colab":{}},"source":["def softmax(state, value, params):\n","  \"\"\"\n","  Softmax policy: selects action probabilistically depending on the value.\n","  Args:\n","    state: an integer corresponding to the current state.\n","    value: a matrix indexed by state and action.\n","    params: a dictionary containing the default parameters.\n","  Returns:\n","    an integer corresponding to the action chosen according to the policy.\n","  \"\"\"\n","  value_now = value[state,:]\n","  # Use params['beta'] (the inverse temperature) which is equal to 1/tau.\n","  prob = exp(value_now * params['beta'])\n","  prob = prob / sum(prob)  # Normalize\n","  cum_prob = cumsum(prob)  # Cumulative sum\n","  action = where(cum_prob > rand())[0][0]\n","  return action"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4YdkxHQ3LyY5","colab_type":"text"},"source":["# Exercise 6: Learning Algorithms\n","\n","**A.** Complete the Q-learning (off-policy) algorithm below.\n","\n","**B.** Modify the Q-learning algorithm to obtain a Sarsa (on-policy) algorithm.\n","\n","Both functions take the previous state, action taken, reward received, value function, current state and default parameters and return the updated value function."]},{"cell_type":"code","metadata":{"id":"IFsm0BzYL89E","colab_type":"code","colab":{}},"source":["def q_learning(prev_state, action, reward, state, value, params):\n","  \"\"\"\n","  Q-learning: updates the value function and returns it.\n","  Args:\n","    prev_state: an integer corresponding to the previous state.\n","    action: an integer corresponding to the action taken.\n","    reward: a float corresponding to the reward received.\n","    state: an integer corresponding to the current state.\n","    value: a matrix indexed by state and action.\n","    params: a dictionary containing the default parameters. \n","  Returns:\n","    the updated value function (matrix indexed by state and action).\n","  \"\"\"\n","  # Maximum value at current state.\n","  if state == None:\n","    max_value = 0\n","  else:\n","    max_value = max(value[state,:])\n","  \n","  # Value of previous state-action pair.\n","  prev_value = value[prev_state, action]\n","  # Reward prediction error. Use params['gamma'], the temporal discount factor.\n","  delta = reward + params['gamma'] * max_value - prev_value  \n","  \n","  # Update value of previous state-action pair. Use params['alpha'], the\n","  # learning rate.\n","  value[prev_state, action] = prev_value + params['alpha'] * delta\n","  \n","  return value"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dpJT9fjgMEPR","colab_type":"code","colab":{}},"source":["def sarsa(prev_state, action, reward, state, value, params):\n","  \"\"\"\n","  Sarsa: updates the value function and returns it.\n","  Args:\n","    prev_state: an integer corresponding to the previous state.\n","    action: an integer correspoding to action taken.\n","    reward: a float corresponding to the reward received.\n","    state: an integer corresponding to the current state.\n","    value: a matrix indexed by state and action.\n","    params: a dictionary containing the default parameters.\n","  Returns:\n","    the updated value function (matrix indexed by state and action).\n","  \"\"\"\n","  # Select the expected value at current state based on our policy by sampling\n","  # from it.\n","  if state == None:\n","    policy_value = 0\n","  else:\n","    # Sample action from the policy for the next state.\n","    policy_action = call_policy(state, value, params)\n","    # Get the value based on the action sampled from the policy.\n","    policy_value = value[state, policy_action]\n","  \n","  # Value of previous state-action pair.\n","  prev_value = value[prev_state, action]\n","  # Reward prediction error. Use params['gamma'], the temporal discount factor.\n","  delta = reward + params['gamma'] * policy_value - prev_value\n","  \n","  # Update value of previous state-action pair. Use params['alpha'], the\n","  # learning rate.\n","  value[prev_state, action] = prev_value + params['alpha'] * delta\n","  \n","  return value"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eRABjR6_Mf-F","colab_type":"text"},"source":["# Exercise 7\n","\n","The code below allows you to select a world, a learning algorithm and a decision policy.\n","\n","**A.** Run 500 episodes (visits to the world) with learning across episodes. Make sure to set a maximum number of steps per episode (e.g. 1000). Use the functions provided in the plot_util module to:\n","* Plot the value associated with each action at each state;\n","* Plot the action corresponding to the maximum value at each state;\n","* Plot the maximum value in each state;\n","* Plot the total reward obtained in each episode.\n","\n","**B.** To make sure that your algorithms have been implemented correctly, compare your results to the ones shown below.\n","\n","**C.** Experiment with different values for the parameters:\n","* Pick a range for the learning rate $\\alpha$ and look at how the results change.\n","* Pick a range for the inverse temperature $\\beta$ (using a softmax policy) and look at how the results change.\n","* Pick a range for $\\epsilon$ (using an $\\epsilon$-greedy policy) and look at how the results change.\n","* Pick a range for the temporal discount factor $\\gamma$ and look at how the results change.\n","\n","**D.** Explore the cliff world with an $\\epsilon$-greedy policy (try $\\epsilon$=0.1) comparing the performance of Q-learning (off-policy) and Sarsa (on-policy). What differences do you notice? What do these differences tell us about on- and off-policy learning?\n","\n","**E.** Compare all of your results with those obtained with the Dynamic Programming algorithms in Exercise 4.\n","\n","* What do you notice about the differences in performance between Dynamic Programming and TD learning algorithms?\n","* What are some of the advantages and disadvantages of each approach?\n","\n","### Cliff world using Q-learning and an $\\epsilon$-greedy policy with $\\epsilon$=0.1 and $\\alpha$=0.3:\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/qlearning_values.png?raw=true\" height=200 width=320>\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/qlearning_actions.png?raw=true\" height=200 width=320>\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/qlearning_maxval.png?raw=true\" height=200 width=320>\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/qlearning_rewards.png?raw=true\" height=200 width=320>\n","\n","### Quentin's world using Sarsa and a softmax policy with $\\beta$=10 and $\\alpha$=0.4:\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/sarsa_values.png?raw=true\" height=200 width=320>\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/sarsa_actions.png?raw=true\" height=200 width=320>\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/sarsa_maxval.png?raw=true\" height=200 width=320>\n","\n","<img src=\"https://github.com/goptavares/intro-to-rl/blob/master/fig/sarsa_rewards.png?raw=true\" height=200 width=320>"]},{"cell_type":"code","metadata":{"id":"IWTDpCc9Huhj","colab_type":"code","colab":{}},"source":["env = cliff_world()\n","# env = quentins_world()\n","# env = windy_cliff_grid()\n","\n","params = default_params(environment=env)\n","\n","# Decision-maker: choose parameter values.\n","params['epsilon'] = 0.01\n","params['alpha'] = 0.5\n","params['beta'] = 10\n","params['gamma'] = 0.8\n","\n","# Choose a policy.\n","params['policy'] = 'epsilon_greedy'\n","# params['policy'] = 'softmax'\n","\n","# Define number of episodes and maximum steps per episode.\n","n_episodes = 1000\n","max_steps = 1000\n","\n","# Initialization.\n","# Start with uniform value function.\n","value = np.ones((env.n_states, env.n_actions))\n","\n","# Run learning.\n","reward_sums, value = run_learning(value, params, n_episodes, max_steps)\n","\n","fig = plot_state_action_values(env, value)\n","fig = plot_quiver_max_action(env, value)\n","fig = plot_heatmap_max_val(env, value)\n","fig = plot_rewards(n_episodes, reward_sums, average_range=10)"],"execution_count":0,"outputs":[]}]}